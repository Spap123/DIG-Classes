{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexical resources, in the context of natural language processing (NLP), refer to various types of linguistic data or databases that provide information about words, their meanings, and their relationships with other words. These resources are essential for NLP tasks such as text analysis, information retrieval, machine translation, and more. Here are some common types of lexical resources:\n",
    "\n",
    "-   Dictionaries: \n",
    "\n",
    "Dictionaries provide definitions, pronunciations, parts of speech, and other lexical information about words. They may also include example sentences, synonyms, antonyms, and usage notes. Examples of dictionaries include WordNet, Wiktionary, and various online dictionaries.\n",
    "\n",
    "-   Thesauri: \n",
    "\n",
    "Thesauri are resources that organize words based on their semantic relationships, such as synonyms (words with similar meanings) and antonyms (words with opposite meanings). They help in finding alternative words and expanding vocabulary. Roget's Thesaurus is a well-known example of a thesaurus.\n",
    "\n",
    "-   Word Lists: \n",
    "\n",
    "Word lists are simple collections of words organized alphabetically or based on specific criteria (e.g., frequency, word length). They serve as basic lexical resources and are used in various NLP tasks such as spell checking, text classification, and word frequency analysis.\n",
    "\n",
    "-   Lexicons: \n",
    "\n",
    "Lexicons are specialized dictionaries or vocabularies that focus on specific domains, languages, or subject areas. They contain terminology, definitions, and linguistic information relevant to their respective domains. For example, a medical lexicon would contain medical terminology and definitions.\n",
    "\n",
    "-   Ontologies: \n",
    "\n",
    "Ontologies are formal representations of knowledge that organize concepts and their relationships in a hierarchical structure. They are used to capture domain-specific knowledge and provide a framework for understanding the semantics of words and concepts. Examples include the Gene Ontology for molecular biology and the Cyc ontology for common-sense reasoning.\n",
    "\n",
    "-   Semantic Networks: \n",
    "\n",
    "Semantic networks represent words and concepts as nodes connected by semantic relationships such as synonymy, hyponymy (hypernymy), meronymy, and entailment. They facilitate semantic analysis and reasoning by capturing the meaning and relationships between words. WordNet is a widely used semantic network.\n",
    "\n",
    "-   Corpora: \n",
    "\n",
    "While not exclusively lexical resources, corpora (plural of corpus) are large collections of text or speech data that serve as valuable sources of lexical information. They provide real-world examples of word usage, collocations, and linguistic patterns, which are essential for various NLP tasks such as text analysis, language modeling, and corpus linguistics research."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dictionary -\n",
    "\n",
    "-   Import NLTK and WordNet:\n",
    "\n",
    "We import NLTK and specifically the wordnet module from NLTK's corpus.\n",
    "\n",
    "-    Specify the Word:\n",
    "\n",
    "We specify the word we want to look up in the dictionary. In this example, we're using the word \"car\".\n",
    "\n",
    "-   Retrieve Synsets:\n",
    "\n",
    "We use the synsets() function from WordNet to retrieve synsets (sets of synonyms) for the specified word.\n",
    "\n",
    "-   Display Information:\n",
    "\n",
    "If synsets are found (i.e., if the word exists in WordNet), we iterate over each synset and display various pieces of information, including the definition, part of speech, examples, and synonyms (lemmas).\n",
    "\n",
    "-   Handling Absence of Synsets:\n",
    "\n",
    "If no synsets are found for the word (i.e., if the word is not in WordNet), we display a message indicating that no information was found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definition: a motor vehicle with four wheels; usually propelled by an internal combustion engine\n",
      "Part of Speech: n\n",
      "Examples: ['he needs a car to get to work']\n",
      "Synonyms: car, auto, automobile, machine, motorcar\n",
      "\n",
      "Definition: a wheeled vehicle adapted to the rails of railroad\n",
      "Part of Speech: n\n",
      "Examples: ['three cars had jumped the rails']\n",
      "Synonyms: car, railcar, railway_car, railroad_car\n",
      "\n",
      "Definition: the compartment that is suspended from an airship and that carries personnel and the cargo and the power plant\n",
      "Part of Speech: n\n",
      "Examples: []\n",
      "Synonyms: car, gondola\n",
      "\n",
      "Definition: where passengers ride up and down\n",
      "Part of Speech: n\n",
      "Examples: ['the car was on the top floor']\n",
      "Synonyms: car, elevator_car\n",
      "\n",
      "Definition: a conveyance for passengers or freight on a cable railway\n",
      "Part of Speech: n\n",
      "Examples: ['they took a cable car to the top of the mountain']\n",
      "Synonyms: cable_car, car\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dictionary\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Word to look up in the dictionary\n",
    "word = \"car\"\n",
    "\n",
    "# Retrieve synsets (sets of synonyms) for the word\n",
    "synsets = wordnet.synsets(word)\n",
    "\n",
    "if synsets:\n",
    "    # Display information for each synset\n",
    "    for synset in synsets:\n",
    "        print(f\"Definition: {synset.definition()}\")\n",
    "        print(f\"Part of Speech: {synset.pos()}\")\n",
    "        print(f\"Examples: {synset.examples()}\")\n",
    "        print(f\"Synonyms: {', '.join(synonym.name() for synonym in synset.lemmas())}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No information found for the word.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### thesauri    \n",
    "    \n",
    "    Import NLTK and WordNet:\n",
    "\n",
    "    We import NLTK and specifically the wordnet module from NLTK's corpus.\n",
    "    Specify the Word:\n",
    "\n",
    "    We specify the word for which we want to find synonyms and antonyms. In this example, we're using the word \"happy\".\n",
    "    Retrieve Synsets:\n",
    "\n",
    "    We use the synsets() function from WordNet to retrieve synsets (sets of synonyms) for the specified word.\n",
    "    Display Synonyms:\n",
    "\n",
    "    If synsets are found (i.e., if the word exists in WordNet), we iterate over each synset and add the synonyms (lemmas) to a set to remove duplicates. Then, we display the set of synonyms for the word.\n",
    "    Display Antonyms:\n",
    "\n",
    "    We also look for antonyms by iterating over each synset, lemma, and antonym. We add the antonyms to a set to remove duplicates and then display the set of antonyms for the word.\n",
    "    Handling Absence of Synsets:\n",
    "\n",
    "    If no synsets are found for the word (i.e., if the word is not in WordNet), we display a message indicating that no synsets were found.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synonyms of 'happy': felicitous, well-chosen, glad, happy\n",
      "Antonyms of 'happy': unhappy\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Word to find synonyms and antonyms for\n",
    "word = \"happy\"\n",
    "\n",
    "# Retrieve synsets (sets of synonyms) for the word\n",
    "synsets = wordnet.synsets(word)\n",
    "\n",
    "if synsets:\n",
    "    # Display synonyms for the word\n",
    "    synonyms = set()\n",
    "    for synset in synsets:\n",
    "        for lemma in synset.lemmas():\n",
    "            synonyms.add(lemma.name())\n",
    "    print(f\"Synonyms of '{word}': {', '.join(synonyms)}\")\n",
    "\n",
    "    # Display antonyms for the word\n",
    "    antonyms = set()\n",
    "    for synset in synsets:\n",
    "        for lemma in synset.lemmas():\n",
    "            for antonym in lemma.antonyms():\n",
    "                antonyms.add(antonym.name())\n",
    "    print(f\"Antonyms of '{word}': {', '.join(antonyms)}\")\n",
    "else:\n",
    "    print(f\"No synsets found for '{word}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopword Lists:\n",
    "\n",
    "Stopwords are common words that are often filtered out during text preprocessing because they typically do not carry significant meaning for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords in English:\n",
      "{\"it's\", 'she', \"wouldn't\", 'itself', 'down', 'against', 'wasn', 'until', 'just', 'but', 'both', \"you've\", 'needn', 'off', 'of', 'if', 'an', \"won't\", 'yourselves', 'its', 'be', 'their', 'he', 'aren', 'yours', 'me', 'then', 'haven', 'through', 'ma', 'again', 'him', 're', 'during', 'll', 'this', 'doing', \"hasn't\", 'ours', 'they', \"aren't\", 'for', 'here', 'further', 'from', \"you'll\", 'himself', 'any', 's', 'mightn', 'isn', \"needn't\", 'after', 'them', 'y', 'ain', 'up', 'having', 't', 'should', 'you', 'will', \"shan't\", 'weren', 'most', 'are', 'what', 'so', \"hadn't\", 'ourselves', 'it', 'those', 'had', 'between', 'each', 'not', 'hadn', 'yourself', 'below', 'doesn', 'very', 'a', 'more', \"you'd\", 'on', 'why', 'wouldn', 'm', 've', \"couldn't\", 'didn', 'does', 'into', 'do', 'only', 'where', 'hers', 'how', \"shouldn't\", 'his', 'some', 'all', 'own', 'have', 'while', 'other', 'nor', 'about', 'o', 'mustn', 'which', \"that'll\", \"weren't\", 'same', \"mustn't\", 'her', 'my', 'themselves', 'am', \"wasn't\", 'theirs', 'or', 'won', 'were', 'than', 'don', 'too', 'hasn', 'herself', \"she's\", 'once', 'shouldn', 'did', 'your', 'under', 'and', \"you're\", 'over', 'whom', 'the', 'who', \"should've\", 'such', \"haven't\", \"don't\", 'd', 'has', \"doesn't\", 'as', 'couldn', 'before', 'no', 'these', \"didn't\", 'in', 'to', 'myself', 'that', 'with', 'our', 'now', 'i', \"mightn't\", 'been', 'can', 'when', 'few', 'at', 'by', 'because', \"isn't\", 'out', 'we', 'is', 'being', 'shan', 'there', 'was', 'above'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sengu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords list\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get English stopwords\n",
    "stopwords_list = set(stopwords.words('english'))\n",
    "\n",
    "# Print the stopwords\n",
    "print(\"Stopwords in English:\")\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Frequency Lists:\n",
    "\n",
    "Word frequency lists contain words along with their frequencies of occurrence in a corpus of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word frequencies:\n",
      "Counter({'it': 2, 'some': 2, 'this': 1, 'is': 1, 'a': 1, 'sample': 1, 'text.': 1, 'contains': 1, 'words,': 1, 'and': 1, 'repeats': 1, 'words.': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Sample text data\n",
    "text = \"This is a sample text. It contains some words, and it repeats some words.\"\n",
    "\n",
    "# Tokenize the text\n",
    "words = text.lower().split()\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq = Counter(words)\n",
    "\n",
    "# Print the most common words and their frequencies\n",
    "print(\"Word frequencies:\")\n",
    "print(word_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lexicons\n",
    "\n",
    "##### Sentiment Lexicon:\n",
    "\n",
    "    A sentiment lexicon contains words along with their associated sentiment polarity (e.g., positive, negative, neutral). It is used in sentiment analysis to determine the sentiment expressed in a piece of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: positive (Score: 7.0)\n"
     ]
    }
   ],
   "source": [
    "# Example: Using the AFINN lexicon for sentiment analysis\n",
    "from afinn import Afinn\n",
    "\n",
    "# Instantiate the AFINN lexicon\n",
    "afinn = Afinn()\n",
    "\n",
    "# Analyze sentiment of a piece of text\n",
    "text = \"This movie is amazing! I love it.\"\n",
    "sentiment_score = afinn.score(text)\n",
    "\n",
    "# Interpret sentiment score\n",
    "if sentiment_score > 0:\n",
    "    sentiment_label = \"positive\"\n",
    "elif sentiment_score < 0:\n",
    "    sentiment_label = \"negative\"\n",
    "else:\n",
    "    sentiment_label = \"neutral\"\n",
    "\n",
    "print(f\"Sentiment: {sentiment_label} (Score: {sentiment_score})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Part-of-Speech (POS) Lexicon:\n",
    "\n",
    "A POS lexicon contains words along with their corresponding parts of speech (e.g., noun, verb, adjective). It is used in POS tagging to assign parts of speech to words in a sentence.\n",
    "\n",
    "    NN: Noun, singular or mass (e.g., \"dog\", \"cat\", \"house\")\n",
    "    NNS: Noun, plural (e.g., \"dogs\", \"cats\", \"houses\")\n",
    "    NNP: Proper noun, singular (e.g., \"John\", \"London\", \"January\")\n",
    "    NNPS: Proper noun, plural (e.g., \"Smiths\", \"United States\", \"Mountains\")\n",
    "    VB: Verb, base form (e.g., \"eat\", \"run\", \"play\")\n",
    "    VBD: Verb, past tense (e.g., \"ate\", \"ran\", \"played\")\n",
    "    VBG: Verb, gerund or present participle (e.g., \"eating\", \"running\", \"playing\")\n",
    "    VBN: Verb, past participle (e.g., \"eaten\", \"run\", \"played\")\n",
    "    VBP: Verb, non-3rd person singular present (e.g., \"eat\", \"run\", \"play\")\n",
    "    VBZ: Verb, 3rd person singular present (e.g., \"eats\", \"runs\", \"plays\")\n",
    "    JJ: Adjective (e.g., \"big\", \"happy\", \"red\")\n",
    "    RB: Adverb (e.g., \"quickly\", \"very\", \"well\")\n",
    "    DT: Determiner (e.g., \"the\", \"a\", \"this\", \"those\")\n",
    "    PRP: Personal pronoun (e.g., \"I\", \"you\", \"he\", \"she\", \"it\", \"they\")\n",
    "    CC: Coordinating conjunction (e.g., \"and\", \"but\", \"or\")\n",
    "    IN: Preposition or subordinating conjunction (e.g., \"in\", \"on\", \"at\", \"since\", \"because\")\n",
    "    CD: Cardinal number (e.g., \"one\", \"two\", \"three\", \"1\", \"2\", \"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sengu\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags:\n",
      "The: DT\n",
      "cat: NN\n",
      "is: VBZ\n",
      "sleeping: VBG\n",
      "on: IN\n",
      "the: DT\n",
      "mat: NN\n",
      ".: .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "# Example: Using the NLTK POS tagger with a sample sentence\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "sentence = \"The cat is sleeping on the mat.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Tag parts of speech\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Print POS tags\n",
    "print(\"POS tags:\")\n",
    "for word, pos_tag in pos_tags:\n",
    "    print(f\"{word}: {pos_tag}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accesing Text Corpora  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    A corpus (plural: corpora) in the context of natural language processing (NLP) refers to a large and structured collection of text or speech data that is used for linguistic analysis, language modeling, and the development and evaluation of NLP algorithms and models. Corpora serve as the primary source of data for studying language patterns, extracting linguistic features, and training machine learning models in NLP tasks.\n",
    "\n",
    "    Here are some key characteristics of corpora:\n",
    "\n",
    "    Size and Diversity: Corpora can vary widely in size, ranging from small collections of texts to large-scale datasets containing millions of documents. They can encompass diverse sources such as books, articles, transcripts, social media posts, websites, and more. The diversity of the corpus influences its representativeness and applicability to different NLP tasks.\n",
    "\n",
    "    Annotation and Metadata: Corpora may contain additional metadata and annotations that provide contextual information about the texts, such as author names, publication dates, genre labels, part-of-speech tags, named entity annotations, sentiment labels, and more. These annotations enhance the usefulness of the corpus for specific NLP tasks and research objectives.\n",
    "\n",
    "    Structured Format: Corpora are typically organized and structured in a standardized format to facilitate data retrieval, processing, and analysis. They may be stored in various formats such as plain text files, XML files, JSON files, database tables, or specialized formats designed for linguistic annotation (e.g., the CoNLL format).\n",
    "\n",
    "    Representativeness and Bias: The representativeness of a corpus refers to how well it reflects the linguistic characteristics and usage patterns of a particular language or domain. Corpora may exhibit biases based on factors such as the selection criteria, data sources, and collection methods. It's important for researchers to be aware of potential biases in corpora and account for them in their analyses and interpretations.\n",
    "\n",
    "    Usage in NLP: Corpora serve as the foundation for many NLP tasks, including text classification, named entity recognition, part-of-speech tagging, sentiment analysis, machine translation, language modeling, and more. Researchers and practitioners leverage corpora to train and evaluate NLP models, extract linguistic insights, develop language resources, and advance the state-of-the-art in NLP.\n",
    "\n",
    "    Overall, corpora play a crucial role in advancing our understanding of natural language and developing effective computational approaches for processing and analyzing text data in various domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Import NLTK and Download Corpus:\n",
    "\n",
    "    import nltk: Import the NLTK library.\n",
    "    nltk.download('gutenberg'): Download the Gutenberg corpus. This is one of many corpora available in NLTK.\n",
    "\n",
    "-   Accessing Text Corpora:\n",
    "\n",
    "    from nltk.corpus import gutenberg: Import the Gutenberg corpus from NLTK's corpus module.\n",
    "\n",
    "-   Listing Available Files:\n",
    "\n",
    "    print(gutenberg.fileids()): Print the list of available files in the Gutenberg corpus. \n",
    "    This provides information about the texts included in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\sengu\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora\\gutenberg.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')  # Download the Gutenberg corpus (one of many available)\n",
    "\n",
    "# Accessing text from the Gutenberg corpus\n",
    "from nltk.corpus import gutenberg\n",
    "\n",
    "# List available files in the Gutenberg corpus\n",
    "print(gutenberg.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Accessing Text from a Specific File:\n",
    "\n",
    "    emma_text = gutenberg.raw('austen-emma.txt'): Access the raw text of a specific file in the Gutenberg corpus (in this case, 'austen-emma.txt', which contains Jane Austen's novel \"Emma\").\n",
    "    \n",
    "-   print(emma_text[:500]): \n",
    "    \n",
    "    Print the first 500 characters of the text to get a glimpse of the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Sense and Sensibility by Jane Austen 1811]\n",
      "\n",
      "CHAPTER 1\n",
      "\n",
      "\n",
      "The family of Dashwood had long been settled in Sussex.\n",
      "Their estate was large, and their residence was at Norland Park,\n",
      "in the centre of their property, where, for many generations,\n",
      "they had lived in so respectable a manner as to engage\n",
      "the general good opinion of their surrounding acquaintance.\n",
      "The late owner of this estate was a single man, who lived\n",
      "to a very advanced age, and who for many years of his life,\n",
      "had a constant companion and housekeeper in his sister.\n",
      "But her death, which happened ten years before his own,\n",
      "produced a great alteration in his home; for to supply\n",
      "her loss, he invited and received into his house the family\n",
      "of his nephew Mr. Henry Dashwood, the legal inheritor\n",
      "of the Norland estate, and the person to whom \n"
     ]
    }
   ],
   "source": [
    "# Accessing the text of a specific file in the Gutenberg corpus\n",
    "emma_text = gutenberg.raw('austen-sense.txt')  # Access the raw text\n",
    "print(emma_text[:800])  # Print the first 500 characters of the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conditional Frequency Distribution \n",
    "\n",
    "Conditional frequency distribution (CFD) is a concept commonly used in Natural Language Processing (NLP) to analyze the distribution of words or other linguistic units within different contexts. It provides a way to examine how the frequency of one variable (e.g., words) varies with respect to another variable (e.g., contexts or categories).\n",
    "\n",
    "In Python, the nltk.ConditionalFreqDist class from the NLTK library is often used to create and work with conditional frequency distributions. Here's how it works:\n",
    "\n",
    "-   Creating a Conditional Frequency Distribution:\n",
    "\n",
    "    To create a conditional frequency distribution, you typically start with a list of tuples where each tuple contains a pair of values representing the condition and the event. Then, you pass this list of tuples to the ConditionalFreqDist constructor.\n",
    "\n",
    "-   Accessing Frequencies:\n",
    "\n",
    "    Once you have created a conditional frequency distribution, you can access the frequencies of specific events conditioned on particular conditions using indexing notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    Alright, imagine you have a bunch of storybooks, and each page of these books has words written on them. Now, let's say you want to know how many times certain words appear in these books, but you also want to know where they appear. That's where a conditional frequency distribution comes in!\n",
    "\n",
    "    Think of it like this: You have a big pile of colorful candies, and you want to see how many candies of each color you have in different jars. The jars represent different situations or conditions. For example, you might want to know how many red candies you have in the jar labeled \"kitchen\" and how many in the jar labeled \"living room.\"\n",
    "\n",
    "    So, in the world of words, we're basically doing the same thing. We're counting how many times specific words (like \"the\" or \"is\") show up in different situations (like when they are used as certain types of words, such as determiners or verbs).\n",
    "\n",
    "    And just like you'd count your candies and put the numbers in each jar, with a conditional frequency distribution, we count the words and store the counts based on their conditions. Then, when we want to know how many times a word appears in a certain situation, we can look it up in the right jar!\n",
    "\n",
    "    So, if you want to know how many times the word \"the\" is used as a determiner (like \"the cat\") in your storybooks, you'd check the jar labeled \"determiner\" and find the number. It helps us understand not only how many times words appear but also where and how they're used in different contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency of toys in different rooms:\n",
      "In the kitchen:\n",
      "- doll: 2 times\n",
      "- ball: 1 times\n",
      "In the living_room:\n",
      "- ball: 2 times\n",
      "- car: 1 times\n",
      "In the bedroom:\n",
      "- doll: 1 times\n",
      "- teddy_bear: 1 times\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Pretend we have a list of toys in different rooms of the house\n",
    "toys_in_rooms = [\n",
    "    ('kitchen', 'ball'),\n",
    "    ('kitchen', 'doll'),\n",
    "    ('living_room', 'ball'),\n",
    "    ('living_room', 'car'),\n",
    "    ('bedroom', 'doll'),\n",
    "    ('bedroom', 'teddy_bear'),\n",
    "    ('kitchen', 'doll'),\n",
    "    ('living_room', 'ball'),\n",
    "]\n",
    "\n",
    "# Create a conditional frequency distribution\n",
    "cfd = nltk.ConditionalFreqDist(toys_in_rooms)\n",
    "\n",
    "# Now, let's see how many times each toy appears in each room\n",
    "print(\"Frequency of toys in different rooms:\")\n",
    "for room in cfd.conditions():  # Get the list of rooms\n",
    "    print(f\"In the {room}:\")\n",
    "    for toy in cfd[room]:  # Get the list of toys in each room\n",
    "        print(f\"- {toy}: {cfd[room][toy]} times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<CategorizedTaggedCorpusReader in 'C:\\\\Users\\\\sengu\\\\AppData\\\\Roaming\\\\nltk_data\\\\corpora\\\\brown'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
