{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\github1\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\github1\\lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\github1\\lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\github1\\lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From d:\\Anaconda\\envs\\github1\\lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "32/32 [==============================] - 1s 3ms/step - loss: 0.6946 - accuracy: 0.5090\n",
      "Epoch 2/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6916 - accuracy: 0.5200\n",
      "Epoch 3/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6903 - accuracy: 0.5360\n",
      "Epoch 4/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6884 - accuracy: 0.5330\n",
      "Epoch 5/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6866 - accuracy: 0.5360\n",
      "Epoch 6/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6851 - accuracy: 0.5460\n",
      "Epoch 7/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6824 - accuracy: 0.5710\n",
      "Epoch 8/10\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.6814 - accuracy: 0.5690\n",
      "Epoch 9/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6798 - accuracy: 0.5870\n",
      "Epoch 10/10\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.6772 - accuracy: 0.5830\n",
      "7/7 [==============================] - 0s 2ms/step - loss: 0.6978 - accuracy: 0.4700\n",
      "Test accuracy: 0.4699999988079071\n",
      "1/1 [==============================] - 0s 100ms/step\n",
      "Predictions for new data:\n",
      "[[0.5053637 ]\n",
      " [0.38357165]\n",
      " [0.5373104 ]\n",
      " [0.5237094 ]\n",
      " [0.53734785]\n",
      " [0.45743692]\n",
      " [0.52177835]\n",
      " [0.51759934]\n",
      " [0.48160008]\n",
      " [0.48362982]]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "# Step 2: Generate synthetic data\n",
    "# Let's create synthetic data for a binary classification problem\n",
    "np.random.seed(0)\n",
    "X_train = np.random.rand(1000, 10)  # 1000 samples with 10 features\n",
    "y_train = np.random.randint(2, size=1000)  # Binary labels (0 or 1)\n",
    "\n",
    "X_test = np.random.rand(200, 10)  # 200 samples for testing\n",
    "y_test = np.random.randint(2, size=200)\n",
    "\n",
    "# Step 3: Build the ANN Model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(10,)),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Step 4: Compile the Model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Step 5: Train the Model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Step 7: Test the Model\n",
    "# Let's generate new synthetic data for testing\n",
    "X_new = np.random.rand(10, 10)  # 10 new samples\n",
    "predictions = model.predict(X_new)\n",
    "print(\"Predictions for new data:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Theory Behind ANN:\n",
    "\n",
    "Artificial Neural Networks (ANNs) are a class of machine learning models inspired by the structure and functioning of the human brain. They consist of interconnected nodes, called neurons, organized in layers. In a typical ANN, there are three types of layers:\n",
    "\n",
    "    Input Layer: Receives input data.\n",
    "    Hidden Layers: Perform computations on the input data.\n",
    "    Output Layer: Produces the final output.\n",
    "\n",
    "Each connection between neurons has a weight associated with it, which determines the strength of influence of one neuron on another. During training, the network adjusts these weights to minimize the difference between the actual and predicted outputs using an optimization algorithm like gradient descent.\n",
    "\n",
    "Steps to Build the ANN Model:\n",
    "\n",
    "    Import Necessary Libraries: We'll need TensorFlow and NumPy for this task.\n",
    "    Generate Synthetic Data: We'll create synthetic data for a simple classification task.\n",
    "    Build the ANN Model: We'll define the architecture of the ANN using TensorFlow's Keras API.\n",
    "    Compile the Model: Specify the loss function, optimizer, and metrics for training.\n",
    "    Train the Model: Fit the model to the synthetic data.\n",
    "    Evaluate the Model: Check the model's performance on the test data.\n",
    "    Test the Model: Use the trained model to make predictions on new data.\n",
    "\n",
    "\n",
    "Activation Functions:\n",
    "Activation functions are mathematical functions applied to the output of each neuron in a neural network. They introduce non-linearities to the network, allowing it to learn complex patterns in the data. Here are some commonly used activation functions:\n",
    "\n",
    "Sigmoid Function:\n",
    "Range: (0, 1)\n",
    "Used in the output layer of binary classification problems.\n",
    "ReLU (Rectified Linear Unit):\n",
    "\n",
    "ReLU(x)=max(0,x)\n",
    "Range: [0, +∞)\n",
    "Most widely used activation function for hidden layers due to its simplicity and effectiveness.\n",
    "\n",
    "Tanh (Hyperbolic Tangent):\n",
    "Range: (-1, 1)\n",
    "Similar to the sigmoid function but centered around zero, often used in hidden layers.\n",
    "\n",
    "Softmax Function:​\n",
    "Range: (0, 1)\n",
    "Used in the output layer of multi-class classification problems to obtain probabilities.\n",
    "\n",
    "Dense Layer:\n",
    "\n",
    "A dense layer, also known as a fully connected layer, is the basic building block of a neural network. It consists of multiple neurons (or nodes), each connected to every neuron in the previous layer, hence \"dense.\" The computation performed in a dense layer is a linear operation followed by an activation function. The number of neurons in the layer determines the output dimensionality.\n",
    "\n",
    "Sequential Model:\n",
    "\n",
    "The sequential model is a linear stack of layers in Keras, a high-level neural networks API running on top of TensorFlow. It allows you to create models layer-by-layer, where each layer has exactly one input tensor and one output tensor. The sequential model is ideal for building simple neural networks where the data flows sequentially from one layer to the next.\n",
    "\n",
    "In the provided code example:\n",
    "\n",
    "keras.Sequential() creates a sequential model.\n",
    "keras.layers.Dense() adds a dense layer to the model.\n",
    "activation parameter in Dense() specifies the activation function to be used.\n",
    "input_shape parameter in the first layer specifies the shape of input data.\n",
    "By stacking dense layers with activation functions, the sequential model transforms the input data through multiple layers to produce output predictions.\n",
    "\n",
    "\n",
    "    Optimizer:\n",
    "    An optimizer is an algorithm used to minimize the loss function during training by adjusting the weights of the neural network. It determines how the weights are updated in each iteration of the training process. Some popular optimizers include:\n",
    "\n",
    "    Gradient Descent: The basic optimization algorithm that adjusts the weights in the direction of the steepest descent of the loss function.\n",
    "\n",
    "    Adam (Adaptive Moment Estimation): An adaptive learning rate optimization algorithm that combines the advantages of two other extensions of gradient descent: AdaGrad and RMSProp. It adapts the learning rates for each parameter based on estimates of first and second moments of the gradients.\n",
    "\n",
    "    SGD (Stochastic Gradient Descent): A variant of gradient descent that updates the weights using a small subset of the training data (mini-batches) at each iteration, making it computationally efficient.\n",
    "\n",
    "    Loss Function (Binary Cross-Entropy):\n",
    "    The loss function, also known as the cost function or objective function, quantifies how well the model's predictions match the actual labels during training. In binary classification tasks (where there are only two classes), binary cross-entropy loss is commonly used. It measures the dissimilarity between the actual and predicted probability distributions of the binary classes.\n",
    "\n",
    "    In the binary cross-entropy loss function, for each instance, the model computes the cross-entropy loss between the true binary label and the predicted probability distribution. The formula for binary cross-entropy loss is:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    Epoch:\n",
    "    An epoch refers to one complete pass through the entire training dataset during the training phase. In other words, one epoch is completed when the model has processed every sample in the training dataset once. Training typically involves running multiple epochs, allowing the model to learn from the data iteratively.\n",
    "\n",
    "    Batch Size:\n",
    "    Batch size refers to the number of training examples utilized in one iteration. Instead of feeding the entire dataset into the model at once, training is typically done in smaller batches. Each batch consists of a subset of the training data, and the model updates its weights based on the gradients computed on this batch.\n",
    "\n",
    "    Using mini-batches (batch size > 1) instead of processing the entire dataset at once offers several advantages:\n",
    "\n",
    "    It reduces memory requirements, especially for large datasets.\n",
    "    It introduces noise into the optimization process, which can help the model generalize better.\n",
    "    It allows for parallelization on hardware like GPUs, leading to faster training times.\n",
    "    Verbose:\n",
    "    The verbose parameter controls how much information about the training process is displayed during training. It accepts different values (0, 1, or 2):\n",
    "\n",
    "    verbose=0: Silent mode. The training process will be silent, and no output will be displayed.\n",
    "    verbose=1: Default mode. Progress bars will be displayed showing the training progress for each epoch.\n",
    "    verbose=2: Verbose mode. One line per epoch will be printed, displaying the progress and the training metrics.\n",
    "    Example Usage in Code:\n",
    "\n",
    "\n",
    "    epochs=10: Specifies that the model should be trained for 10 epochs.\n",
    "    batch_size=32: Specifies that each training iteration should process 32 samples at a time.\n",
    "    verbose=1: Specifies that progress bars will be displayed during training.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
