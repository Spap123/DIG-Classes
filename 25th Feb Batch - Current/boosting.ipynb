{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boosting:\n",
    "\n",
    "Boosting is like having a group of friends, and each friend is good at different things. You ask each friend to help with the part they're good at, and then you combine their strengths to achieve a better overall result.\n",
    "\n",
    "In machine learning, boosting builds a series of models sequentially, where each new model corrects the errors of the previous one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AdaBoost (Adaptive Boosting):\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Imagine you are learning to play a game, and your friend gives you clues to help you get better. If you make mistakes, your friend pays more attention to those mistakes and helps you improve on them. AdaBoost works similarly with machines. It creates a team of different players (weak learners) and pays more attention to the mistakes they make, helping the team get better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a base decision tree classifier\n",
    "base_classifier = DecisionTreeClassifier(max_depth=1)  # Shallow tree as a weak learner\n",
    "\n",
    "# Create an AdaBoost classifier with 50 base classifiers\n",
    "adaboost_classifier = AdaBoostClassifier(base_classifier, n_estimators=50, random_state=42)\n",
    "\n",
    "# Train the AdaBoost classifier\n",
    "adaboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = adaboost_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"AdaBoost Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting:\n",
    "\n",
    "Explanation:\n",
    "\n",
    "Imagine you are climbing a hill, and each time you take a step, your friend tells you how far you are from the top. You adjust your next step based on the feedback to get closer. Gradient Boosting is like this; it builds a series of helpers (weak learners) where each one guides the next to reduce the distance to the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Gradient Boosting classifier\n",
    "gradient_boosting_classifier = GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train the Gradient Boosting classifier\n",
    "gradient_boosting_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = gradient_boosting_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Gradient Boosting Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost (Extreme Gradient Boosting):\n",
    "\n",
    "Explanation:\n",
    "\n",
    "XGBoost is like having a superhero friend who not only helps you climb the hill but also has a better strategy and learns from each step. It's faster and smarter, making the climbing process more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an XGBoost classifier\n",
    "xgboost_classifier = xgb.XGBClassifier(n_estimators=50, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# Train the XGBoost classifier\n",
    "xgboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = xgboost_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"XGBoost Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### LightGBM (Light Gradient Boosting Machine):\n",
    "\n",
    "Explanation:\n",
    "\n",
    "LightGBM is like having a friendly robot guide that uses a different method to climb the hill. It groups similar steps together and makes decisions faster, making the climbing process even quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Load the iris dataset\n",
    "# iris = load_iris()\n",
    "# X, y = iris.data, iris.target\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create a LightGBM classifier\n",
    "# lightgbm_classifier = lgb.LGBMClassifier(n_estimators=50, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# # Train the LightGBM classifier\n",
    "# lightgbm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = lightgbm_classifier.predict(X_test)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(y_test, predictions)\n",
    "# print(\"LightGBM Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CatBoost (Categorical Boosting):\n",
    "\n",
    "Explanation:\n",
    "\n",
    "CatBoost is like having a friend who understands different languages and helps you climb the hill without needing to translate. It handles different types of information more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from catboost import CatBoostClassifier\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Load the iris dataset\n",
    "# iris = load_iris()\n",
    "# X, y = iris.data, iris.target\n",
    "\n",
    "# # Split the data into training and testing sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Create a CatBoost classifier\n",
    "# catboost_classifier = CatBoostClassifier(n_estimators=50, learning_rate=0.1, random_state=42)\n",
    "\n",
    "# # Train the CatBoost classifier\n",
    "# catboost_classifier.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions\n",
    "# predictions = catboost_classifier.predict(X_test)\n",
    "\n",
    "# # Calculate accuracy\n",
    "# accuracy = accuracy_score(y_test, predictions)\n",
    "# print(\"CatBoost Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
