{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m XGBRegressor\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LGBMRegressor\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Set a seed for reproducibility\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from prophet import Prophet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate random time series data\n",
    "date_rng = pd.date_range(start='2022-01-01', end='2022-12-31', freq='D')\n",
    "data = np.random.randn(len(date_rng))\n",
    "ts_df = pd.DataFrame(data, columns=['value'], index=date_rng)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(ts_df) * 0.8)\n",
    "train, test = ts_df[0:train_size], ts_df[train_size:]\n",
    "\n",
    "# Function to evaluate the model and print RMSE\n",
    "def evaluate_model(model, test, pred):\n",
    "    mse = mean_squared_error(test, pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f'Model: {model}, RMSE: {rmse}')\n",
    "\n",
    "# SARIMA model\n",
    "sarima_model = SARIMAX(train, order=(1, 1, 1), seasonal_order=(1, 1, 1, 7))\n",
    "sarima_fit = sarima_model.fit(disp=False)\n",
    "sarima_pred = sarima_fit.get_forecast(steps=len(test)).predicted_mean\n",
    "evaluate_model('SARIMA', test, sarima_pred)\n",
    "\n",
    "# Exponential Smoothing (ETS) model\n",
    "ets_model = ExponentialSmoothing(train, seasonal='add', seasonal_periods=7)\n",
    "ets_fit = ets_model.fit()\n",
    "ets_pred = ets_fit.predict(start=test.index[0], end=test.index[-1])\n",
    "evaluate_model('ETS', test, ets_pred)\n",
    "\n",
    "# Prophet model\n",
    "prophet_model = Prophet()\n",
    "prophet_train = pd.DataFrame({'ds': train.index, 'y': train['value']})\n",
    "prophet_model.fit(prophet_train)\n",
    "future = pd.DataFrame({'ds': test.index})\n",
    "prophet_pred = prophet_model.predict(future)['yhat'].values\n",
    "evaluate_model('Prophet', test, prophet_pred)\n",
    "\n",
    "# Random Forest model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(np.arange(len(train)).reshape(-1, 1), train['value'])\n",
    "rf_pred = rf_model.predict(np.arange(len(test)).reshape(-1, 1))\n",
    "evaluate_model('Random Forest', test, rf_pred)\n",
    "\n",
    "# XGBoost model\n",
    "xgb_model = XGBRegressor(n_estimators=100, random_state=42)\n",
    "xgb_model.fit(np.arange(len(train)).reshape(-1, 1), train['value'])\n",
    "xgb_pred = xgb_model.predict(np.arange(len(test)).reshape(-1, 1))\n",
    "evaluate_model('XGBoost', test, xgb_pred)\n",
    "\n",
    "# LightGBM model\n",
    "lgbm_model = LGBMRegressor(n_estimators=100, random_state=42)\n",
    "lgbm_model.fit(np.arange(len(train)).reshape(-1, 1), train['value'])\n",
    "lgbm_pred = lgbm_model.predict(np.arange(len(test)).reshape(-1, 1))\n",
    "evaluate_model('LightGBM', test, lgbm_pred)\n",
    "\n",
    "# LSTM model\n",
    "def create_lstm_model():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(1, 1)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "lstm_model = create_lstm_model()\n",
    "lstm_train = train['value'].values.reshape(-1, 1, 1)\n",
    "lstm_model.fit(lstm_train, train['value'], epochs=50, verbose=0)\n",
    "lstm_pred = lstm_model.predict(np.arange(len(test)).reshape(-1, 1, 1)).flatten()\n",
    "evaluate_model('LSTM', test, lstm_pred)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
