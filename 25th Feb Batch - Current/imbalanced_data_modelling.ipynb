{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Topics Being Covered \n",
    "\n",
    "*   Balancing the Data using Resampling Methods\n",
    "*   Fixing a Problem in the Real World\n",
    "*   Getting Data Ready for Predictive Modeling\n",
    "*   Making use of Sklearn Logistic Regression\n",
    "*   Using Sklearn to apply Random Forest\n",
    "*   Using Imblearn to implement Random Over Sampling\n",
    "*   Using Imblearn to implement Random Under Sampling\n",
    "*   Imblearn implementation of synthetic sampling\n",
    "*   Using Imblearn to implement a Neighbor-based Sampling that Combines Oversampling and Undersampling\n",
    "*   Ensemble Models for Imbalanced Data Implementation\n",
    "*   XG Boost for Imbalanced Data Introduction\n",
    "*   Comparison of the Outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   What is an Imbalanced Dataset?\n",
    "\n",
    "        Imagine you have a box of colored candies. Most of them are red, but there are only a few green candies. That's like having an imbalanced dataset! In the world of data and numbers, a dataset is a collection of information. When we say it's imbalanced, it means one type of thing (like red candies) is much more common than the other (like green candies).\n",
    "\n",
    "        Now, let's create a simple example using Python and generate an imbalanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# Create an imbalanced dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, \n",
    "    n_features=2, \n",
    "    n_informative=2, \n",
    "    n_redundant=0, \n",
    "    n_clusters_per_class=1, \n",
    "    weights=[0.95], \n",
    "    flip_y=0, \n",
    "    random_state=1\n",
    ")\n",
    "\n",
    "# Convert to DataFrame for easy visualization\n",
    "df = pd.DataFrame(X, columns=['Feature1', 'Feature2'])\n",
    "df['Label'] = y\n",
    "\n",
    "# Let's see the first few rows of the dataset\n",
    "df.head()\n",
    "df.to_csv('imbalanced.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a dataset with 1000 samples.\n",
    "There are two features (like the color and size of candies).\n",
    "We made it imbalanced by setting the weights parameter to [0.95], meaning 95% of the samples belong to one class.\n",
    "The result is a DataFrame with features ('Feature1' and 'Feature2') and labels ('Label')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution Before Resampling:\n",
      "0    950\n",
      "1     50\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Class Distribution After Resampling:\n",
      "0    950\n",
      "1    950\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "print(\"Class Distribution Before Resampling:\")\n",
    "print(df['Label'].value_counts())\n",
    "\n",
    "# Balance the dataset using Random Over Sampling\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Convert resampled data to DataFrame\n",
    "df_resampled = pd.DataFrame(X_resampled, columns=['Feature1', 'Feature2'])\n",
    "df_resampled['Label'] = y_resampled\n",
    "\n",
    "# Check the class distribution after resampling\n",
    "print(\"\\nClass Distribution After Resampling:\")\n",
    "print(df_resampled['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first print the class distribution before resampling to see the imbalance.\n",
    "Then, we use RandomOverSampler from the imbalanced-learn library to balance the dataset.\n",
    "After resampling, we print the class distribution again to see that both classes now have similar counts.\n",
    "\n",
    "Imagine you have two baskets of candies. In one basket, you have lots of red candies, and in the other basket, you only have a few green candies. Now, let's say you want to make it fair and have a similar number of both red and green candies because you love both colors.\n",
    "\n",
    "Here's what oversampling is like:\n",
    "\n",
    "Look at Your Candy Baskets:\n",
    "\n",
    "You have two baskets, one with many red candies and another with only a few green candies.\n",
    "Make More Green Candies:\n",
    "\n",
    "To make it fair, you decide to create more green candies. You don't change the existing red candies; you just add more green candies to the basket with fewer of them.\n",
    "Now You Have Balanced Baskets:\n",
    "\n",
    "After adding more green candies, both baskets have a similar number of red and green candies.\n",
    "In the language of computers and data, oversampling is like making more copies of the less common thing (like green candies) so that you have a fair amount of both. This helps computer programs learn about both red and green candies equally well.\n",
    "\n",
    "In the code example I provided earlier, we did the same thing with a dataset. We had more of one type of data, and we used a special computer tool (Random Over Sampling) to make more copies of the less common data, so the computer can learn about both types equally. It's like making sure both red and green candies get the attention they deserve!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready for modelling the data , we will use LR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9289473684210526\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.95      0.93       190\n",
      "           1       0.95      0.91      0.93       190\n",
      "\n",
      "    accuracy                           0.93       380\n",
      "   macro avg       0.93      0.93      0.93       380\n",
      "weighted avg       0.93      0.93      0.93       380\n",
      "\n",
      "Confusion Matrix:\n",
      "[[180  10]\n",
      " [ 17 173]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a logistic regression model\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Train the model on the training data\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained model to make predictions on the test data\n",
    "lr_predictions = lr_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, lr_predictions)\n",
    "report = classification_report(y_test, lr_predictions)\n",
    "matrix = confusion_matrix(y_test, lr_predictions)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution Before Resampling:\n",
      "0    950\n",
      "1     50\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Class Distribution After Resampling:\n",
      "0    950\n",
      "1    950\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Using Imblearn to implement Random Over Sampling\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "df = pd.read_csv(r'imbalanced.csv')\n",
    "# Check the class distribution before resampling\n",
    "print(\"Class Distribution Before Resampling:\")\n",
    "print(df['Label'].value_counts())\n",
    "\n",
    "# Balance the dataset using Random Over Sampling\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "\n",
    "# Convert resampled data to DataFrame\n",
    "df_resampled = pd.DataFrame(X_resampled, columns=['Feature1', 'Feature2'])\n",
    "df_resampled['Label'] = y_resampled\n",
    "\n",
    "# Check the class distribution after resampling\n",
    "print(\"\\nClass Distribution After Resampling:\")\n",
    "print(df_resampled['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution Before Resampling:\n",
      "0    950\n",
      "1     50\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Class Distribution After Resampling:\n",
      "0    50\n",
      "1    50\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Using Imblearn to implement Random Under Sampling\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "df = pd.read_csv(r'imbalanced.csv')\n",
    "# Check the class distribution before resampling\n",
    "print(\"Class Distribution Before Resampling:\")\n",
    "print(df['Label'].value_counts())\n",
    "\n",
    "# Balance the dataset using Random Under Sampling\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "\n",
    "# Convert resampled data to DataFrame\n",
    "df_resampled = pd.DataFrame(X_resampled, columns=['Feature1', 'Feature2'])\n",
    "df_resampled['Label'] = y_resampled\n",
    "\n",
    "# Check the class distribution after resampling\n",
    "print(\"\\nClass Distribution After Resampling:\")\n",
    "print(df_resampled['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution Before Resampling:\n",
      "0    950\n",
      "1     50\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Class Distribution After Resampling:\n",
      "0    950\n",
      "1    950\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Imblearn implementation of synthetic sampling\n",
    "from imblearn.over_sampling import SMOTE\n",
    "df = pd.read_csv(r'imbalanced.csv')\n",
    "# Check the class distribution before resampling\n",
    "print(\"Class Distribution Before Resampling:\")\n",
    "print(df['Label'].value_counts())\n",
    "\n",
    "# Balance the dataset using Synthetic Minority Over-sampling Technique (SMOTE)\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Convert resampled data to DataFrame\n",
    "df_resampled = pd.DataFrame(X_resampled, columns=['Feature1', 'Feature2'])\n",
    "df_resampled['Label'] = y_resampled\n",
    "\n",
    "# Check the class distribution after resampling\n",
    "print(\"\\nClass Distribution After Resampling:\")\n",
    "print(df_resampled['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Distribution Before Resampling:\n",
      "0    950\n",
      "1     50\n",
      "Name: Label, dtype: int64\n",
      "\n",
      "Class Distribution After Resampling:\n",
      "1    853\n",
      "0    837\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Using Imblearn to implement a Neighbor-based Sampling that Combines Oversampling and Undersampling\n",
    "from imblearn.combine import SMOTEENN\n",
    "df = pd.read_csv(r'imbalanced.csv')\n",
    "# Check the class distribution before resampling\n",
    "print(\"Class Distribution Before Resampling:\")\n",
    "print(df['Label'].value_counts())\n",
    "\n",
    "# Balance the dataset using SMOTEENN (combination of SMOTE and Edited Nearest Neighbors)\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_resampled, y_resampled = smote_enn.fit_resample(X, y)\n",
    "\n",
    "# Convert resampled data to DataFrame\n",
    "df_resampled = pd.DataFrame(X_resampled, columns=['Feature1', 'Feature2'])\n",
    "df_resampled['Label'] = y_resampled\n",
    "\n",
    "# Check the class distribution after resampling\n",
    "print(\"\\nClass Distribution After Resampling:\")\n",
    "print(df_resampled['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Random Over Sampling (ROS):\n",
    "    Imagine you have a box of candies, and most of them are red, but you want more of the rare green candies. So, you decide to make some extra green candies to balance things out. Now, you have more of both colors!\n",
    "\n",
    "    Random Under Sampling (RUS):\n",
    "    Picture another box of candies, but this time you have a lot of green candies and just a few red ones. To make it fair, you decide to remove some green candies until you have an equal number of both colors. Now, both colors have the same amount.\n",
    "\n",
    "    Synthetic Sampling (SMOTE):\n",
    "    Now, think of a box with mostly red candies and only a few green candies. With synthetic sampling, you imagine new candies in between the existing green candies. It's like magically creating more green candies, so you end up with a balanced mix of red and green candies.\n",
    "\n",
    "    Neighbor-based Sampling (SMOTEENN):\n",
    "    In this case, you have red and green candies, but you don't just want more green candies or less green candies. You want to make sure every green candy has some red candies nearby. So, you add new green candies in a way that keeps them close to the existing red candies. This way, the colors are balanced, and each green candy has red friends.\n",
    "\n",
    "    So, in simple terms:\n",
    "\n",
    "    Random Over Sampling: Make more of the less common thing.\n",
    "    Random Under Sampling: Take away some of the more common thing.\n",
    "    Synthetic Sampling (SMOTE): Create more of the less common thing magically.\n",
    "    Neighbor-based Sampling (SMOTEENN): Make sure every less common thing has friends nearby."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
